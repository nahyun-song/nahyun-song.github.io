---
layout: single
title:  "20220228 week2-1"
categories: preonboarding
---

# Week2-1 / Pytorch Tutorial (1)

1. tensor 생성
    - `torch` : 텐서를 생성하는 라이브러리
    - `torch.autograd` : 자동미분 기능을 제공하는 라이브러리
    - `torch.nn` : 신경망을 생성하는 라이브러리
    - `torch.multiprocessing` : 병렬처리 기능을 제공하는 라이브러리
    - `torch.utils` : 데이터 조작 등 유틸리티 기능 제공
    - `torch.legacy` : Torch로부터 포팅해온 코드
    - `torch.onnx` : Open Neural Network Exchange
        - 서로 다른 프레임워크 간의 모델을 공유할 때 사용
2. tensor 정보
    - 넘파이의 n-dimesional array와 유사
    - GPU를 사용한 연산 가속도 가능

3. tensor 인덱싱
    - numpy처럼 인덱싱 사용 가능
    

4. tensor 연산
    - 덧셈 :  x+y, `torch.add(x,y)`, in-place 방식
        - `in-place` : y += x 와 같은 형태. `x.copy_(y)`, `x.t_()`
    - 뺄셈 : `torch.sub`
    - 곱셉 : `torch.mul`
    - 나눗셈 : `torch.div`
    - 내적 : `torch.mm`

5. 텐서 구성 : [레이어개수, 배치크기(문장의 개수), 토큰의 개수(문장의 길이), hidden_size(한 토큰의 차원수)]



```python
!pip3 install torch torchvision torchaudio
```

    Collecting torch
      Downloading torch-1.10.2-cp38-none-macosx_10_9_x86_64.whl (147.2 MB)
    [K     |████████████████████████████████| 147.2 MB 4.1 MB/s eta 0:00:011    |█████████████▉                  | 63.8 MB 8.3 MB/s eta 0:00:11
    [?25hCollecting torchvision
      Downloading torchvision-0.11.3-cp38-cp38-macosx_10_9_x86_64.whl (1.2 MB)
    [K     |████████████████████████████████| 1.2 MB 11.6 MB/s eta 0:00:01
    [?25hCollecting torchaudio
      Downloading torchaudio-0.10.2-cp38-cp38-macosx_10_9_x86_64.whl (2.4 MB)
    [K     |████████████████████████████████| 2.4 MB 9.7 MB/s eta 0:00:01
    [?25hRequirement already satisfied: typing-extensions in /Users/nahyunsong/opt/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.3)
    Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /Users/nahyunsong/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (8.0.1)
    Requirement already satisfied: numpy in /Users/nahyunsong/opt/anaconda3/lib/python3.8/site-packages (from torchvision) (1.19.2)
    Installing collected packages: torch, torchvision, torchaudio
    Successfully installed torch-1.10.2 torchaudio-0.10.2 torchvision-0.11.3



```python
# 2. 연습
import torch
```


```python
torch.__version__
```




    '1.10.2'




```python
# 초기화 되지 않은 행렬
x = torch.empty(4, 2) #shpae이 4x2인 tensor 생성
print(x)
```

    tensor([[5.2286e+25, 4.5912e-41],
            [5.2581e+25, 4.5912e-41],
            [5.2985e+25, 4.5912e-41],
            [5.2547e+25, 4.5912e-41]])



```python
#무작위로 초기화된 행렬
x = torch.rand(4,2)
print(x)
```

    tensor([[0.2214, 0.3889],
            [0.7894, 0.2470],
            [0.6957, 0.6837],
            [0.9224, 0.1818]])



```python
#dtype이 long(datatype), 0으로 채워진 텐서
x = torch.zeros(4,2,dtype=torch.long)
print(x)
```

    tensor([[0, 0],
            [0, 0],
            [0, 0],
            [0, 0]])



```python
#텐서 직접 만들기
x = torch.tensor([3, 2.3])
print(x)
```

    tensor([3.0000, 2.3000])



```python
x = x.new_ones(2,4, dtype=torch.double) #double = float64
print(x)
```

    tensor([[1., 1., 1., 1.],
            [1., 1., 1., 1.]], dtype=torch.float64)



```python
#x의 모양은 그대로 가져오되 값은 random으로 채움
x = torch.randn_like(x, dtype=torch.float)
print(x)
```

    tensor([[-0.8325, -0.3986,  0.0660,  2.4912],
            [-0.7553,  1.3947, -0.7718, -0.6213]])



```python
#텐서의 크기
print(x.size())
```

    torch.Size([2, 4])



```python
#덧셈
y = torch.rand(2,4)
print(x+y)
```

    tensor([[-0.5721,  0.4136,  0.6276,  3.4804],
            [ 0.0536,  2.1490,  0.1260, -0.5015]])



```python
print(torch.add(x, y))
```

    tensor([[-0.5721,  0.4136,  0.6276,  3.4804],
            [ 0.0536,  2.1490,  0.1260, -0.5015]])



```python
result = torch.empty(2, 4)
torch.add(x, y, out=result)
print(result)
```

    tensor([[-0.5721,  0.4136,  0.6276,  3.4804],
            [ 0.0536,  2.1490,  0.1260, -0.5015]])



```python
print(x)
print(y)
y.add_(x)
print(y)
```

    tensor([[-0.8325, -0.3986,  0.0660,  2.4912],
            [-0.7553,  1.3947, -0.7718, -0.6213]])
    tensor([[0.2604, 0.8122, 0.5616, 0.9892],
            [0.8089, 0.7543, 0.8978, 0.1197]])
    tensor([[-0.5721,  0.4136,  0.6276,  3.4804],
            [ 0.0536,  2.1490,  0.1260, -0.5015]])



```python
# 뺄셈
x = torch.Tensor([[1, 3],
                 [5, 7]])
y = torch.Tensor([[2, 4],
                 [6, 8]])
print(x-y)
print(torch.sub(x, y))
print(x.sub(y))
```

    tensor([[-1., -1.],
            [-1., -1.]])
    tensor([[-1., -1.],
            [-1., -1.]])
    tensor([[-1., -1.],
            [-1., -1.]])



```python
#곱셈 (각 원소를 곱합. 행렬곱X)
x = torch.Tensor([[1, 3],
                 [5, 7]])
y = torch.Tensor([[2, 4],
                 [6, 8]])
print(x*y)
print(torch.mul(x, y))
print(x.mul(y))
```

    tensor([[ 2., 12.],
            [30., 56.]])
    tensor([[ 2., 12.],
            [30., 56.]])
    tensor([[ 2., 12.],
            [30., 56.]])



```python
#나눗셈
x = torch.Tensor([[1, 3],
                 [5, 7]])
y = torch.Tensor([[2, 4],
                 [6, 8]])
print(x/y)
print(torch.div(x, y))
print(x.div(y))
```

    tensor([[0.5000, 0.7500],
            [0.8333, 0.8750]])
    tensor([[0.5000, 0.7500],
            [0.8333, 0.8750]])
    tensor([[0.5000, 0.7500],
            [0.8333, 0.8750]])



```python
#내적 (행렬곱)
x = torch.Tensor([[1, 3],
                 [5, 7]])
y = torch.Tensor([[2, 4],
                 [6, 8]])
print(torch.mm(x, y))
```

    tensor([[20., 28.],
            [52., 76.]])



```python
#인덱싱
print(x)
print(x[:,1])
```

    tensor([[1., 3.],
            [5., 7.]])
    tensor([3., 7.])



```python
# view - 텐서의 크기나 모양을 변경
x = torch.randn(4, 5)
y = x.view(20)
z = x.view(5, -1) # -1 : 자동

print(x)
print(y)
print(z)
```

    tensor([[ 0.6993,  0.3697,  0.1376, -0.6983,  1.2791],
            [-1.1951,  1.3451,  1.2371,  0.4501, -0.3160],
            [ 1.0536, -0.1442,  1.7443, -0.1244, -1.2180],
            [ 1.2323, -1.8784,  0.0217, -0.5873, -0.9292]])
    tensor([ 0.6993,  0.3697,  0.1376, -0.6983,  1.2791, -1.1951,  1.3451,  1.2371,
             0.4501, -0.3160,  1.0536, -0.1442,  1.7443, -0.1244, -1.2180,  1.2323,
            -1.8784,  0.0217, -0.5873, -0.9292])
    tensor([[ 0.6993,  0.3697,  0.1376, -0.6983],
            [ 1.2791, -1.1951,  1.3451,  1.2371],
            [ 0.4501, -0.3160,  1.0536, -0.1442],
            [ 1.7443, -0.1244, -1.2180,  1.2323],
            [-1.8784,  0.0217, -0.5873, -0.9292]])



```python
#item - 스칼라 값이 하나만 존재해야 함!
x = torch.randn(1)
print(x)
print(x.item())
print(x.dtype)
```

    tensor([-0.8566])
    -0.8565942645072937
    torch.float32



```python
#squeeze - 차원 축소 (제거)
tensor = torch.rand(1, 3, 3)
print(tensor)
tensor.shape
```

    tensor([[[0.8683, 0.1970, 0.8002],
             [0.5062, 0.3201, 0.6410],
             [0.3254, 0.1921, 0.8986]]])





    torch.Size([1, 3, 3])




```python
t = tensor.squeeze()
print(t)
print(t.shape)
```

    tensor([[0.3778, 0.3723, 0.6826],
            [0.5726, 0.0437, 0.7443],
            [0.5970, 0.1065, 0.5964]])
    torch.Size([3, 3])



```python
#unsqueeze - 차원 증가 (생성)
tensor = torch.rand(1, 3, 3)
print(tensor)
tensor.shape
```

    tensor([[[0.0014, 0.5739, 0.8979],
             [0.5243, 0.9613, 0.6927],
             [0.2039, 0.4004, 0.6117]]])





    torch.Size([1, 3, 3])




```python
t = tensor.unsqueeze(dim=0)
print(t)
t.shape
```

    tensor([[[[0.0014, 0.5739, 0.8979],
              [0.5243, 0.9613, 0.6927],
              [0.2039, 0.4004, 0.6117]]]])





    torch.Size([1, 1, 3, 3])




```python
#stack - 텐서간 결합
x = torch.FloatTensor([1, 4])
y = torch.FloatTensor([2, 5])
z = torch.FloatTensor([3, 6])

print(torch.stack([x, y, z]))
```

    tensor([[1., 4.],
            [2., 5.],
            [3., 6.]])



```python
#cat : 텐서를 결합하는 메서드(concatenate)
## numpy의 stack과 유사하지만, 쌓을 dim이 존재해야함 - 해당 차원을 늘려준 후 결합
a = torch.randn(1, 1, 3, 3)
b = torch.randn(1, 1, 3, 3)
c = torch.cat((a, b), dim=0) #dimension : Concat 기준점!

print(c)
c.size() #dim=0이기 때문에 토치 사이즈를 출력한 값에서 0번 인덱스가 늘어난 것을 확인
```

    tensor([[[[-2.0085,  1.5450,  2.2465],
              [ 1.4097, -0.4118,  0.6842],
              [ 1.5219, -0.2504, -1.4198]]],
    
    
            [[[-1.5226, -1.1809,  1.0983],
              [-0.1676, -0.8545, -0.5558],
              [-1.8932,  1.2518, -1.4177]]]])





    torch.Size([2, 1, 3, 3])




```python
#torch, numpy 호환 : numpy(), from_numpy()
## tensor가 CPU상에 있다면 numpy 배열은 메모리 공간을 공유하므로 하나가 변하면, 다른 하나도 변함

a = torch.ones(7)
print(a)

```

    tensor([1., 1., 1., 1., 1., 1., 1.])



```python
b = a.numpy()
print(b)
```

    [1. 1. 1. 1. 1. 1. 1.]



```python
a.add_(1)
print(a)
b #CPU상에 있으므로 서로 공유하여 값이 같이 변함
```

    tensor([2., 2., 2., 2., 2., 2., 2.])





    array([2., 2., 2., 2., 2., 2., 2.], dtype=float32)




```python
import numpy as np
a = np.ones(7)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
print(a)
b
```

    [2. 2. 2. 2. 2. 2. 2.]





    tensor([2., 2., 2., 2., 2., 2., 2.], dtype=torch.float64)


