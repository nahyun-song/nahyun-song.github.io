---
layout: single
title:  "20220404 문장을 작은 단위로 쪼개기"
categories: preonboarding
---

## 토큰화란?
1. 토큰화 : 문장을 토큰 시퀀스로 나누는 과정. 문자, 단어, 서브워드 등을 기준으로
2. 토크나이저 : 토큰화 + 품사부착을 수행하는 프로그램 ex) mecab, kkma
3. 단어 단위 토큰화
    - 공백으로 분리 : 별도로 토크나이저를 쓰지 않아도 된다.
    - 어휘 집합의 크기가 커질 수 있다 -> 모델 학습이 어려워 질 수 있음
4. 문자 단위 토큰화
    - 모든 문자를 어휘집합에 포함하므로 미등록 토큰 문제로부터 자유롭다.
    - 각 문자 토큰은 의미있는 단위가 되기 어렵다.
    - 토큰 시퀀스의 길이가 단어 단위에 비해 길다 -> 학습어려움 -> 성능 떨어짐
5. 서브워드 단위 토큰화
    - 단어와 문자 단위 토큰화의 중간 형태 ex) Byte Pair Encoding

## 바이트 페어 인코딩이란?
1. BPE : 정보 압축 알고리즘으로, 데이터에서 가장 많이 등장한 문자열을 병합해서 데이터를 압축하는 기법
    - 사전 크기 증가를 억제하면서도 정보를 효율적으로 압출할 수 있는 알고리즘
    - 말뭉치에서 자주 나타나는 서브워드를 토큰으로 분석하기 떄문에 분석 대상 언어에 대한 지식이 필요 없다
2. BPE를 활용한 토큰화 절차
    1. 어휘 집합 구축 : 자주 등장하는 문자열을 병합하고 이를 어휘집합에 추가 (원하는 집합 크기가 될 때까지)
    2. 토큰화 : 대상 문장의 각 어절에 어휘집합에 있는 서브워드가 포함되었을 때, 해당 서브워드를 어절에서 분리
3. BPE 어휘집합 구축하기
    - 말뭉치 준비 -> 말뭉치의 모든 문장을 공백으로 나눔 -> 고빈도 바이그램 쌍을 원하는 크기가 될 때까지 병합
4. 워드피스
    - 말뭉치에서 자주 등장하는 문자열을 토큰으로 인식
    - 빈도가 아니라 우도를 높이는 방향으로 쌍을 병합
    - 병합 후보에 오른 쌍을 미리 병합해 보고 잃는 것과 가치 등을 판단한 후에 병합
    
## 토큰화하기
1. GPT 모델 입력 만들기
```
batch_inputs = tokenizer_gpt(sentences,
                                padding = 'max_length', # 문장의 최대 길이에 맞춰 패딩
                                max_length = 12, # 문장의 토큰 기준 최대 길이
                                truncation = True) # 문장 잘림 허용 옵션
```
- 실행결과
    1. inputs_ids : 토큰화 결과를 가지고 각 토큰을 인덱스로 바꾼 것
        - [PAD] = 0 더미 토큰으로 길이를 맞춰주는 역할
    2. attention_mask : 일반토큰(1) 패딩토큰(0)의 자리를 구분해서 알려줌
    
2. BERT 모델 입력 만들기
```
batch_inputs = tokenizer_bert(sentences,
                                padding = 'max_length', 
                                max_length = 12, 
                                truncation = True)
```
- 실행결과
    1. inputs_ids 
        - [CLS] = 2 문장의 시작
        - [SEP] = 3 문장의 끝 
        둘 다 max_length 안에 들어감
    2. attention_mask
        - [CLS], [SEP] : 일반 토큰 취급 (1)
    3. token_type_ids : 세그먼트(문서 또는 문장)을 나타냄
