---
layout: single
title:  "20211108 인공신경망 / MLP / 경사하강법 / 오차역전파"
categories: AI_bootcamp
---

## 1. 퍼셉트론(Perceptron)
1. 퍼셉트론 : 가장 간단한 인공 신경망 구조 중 하나
- 입력층과 출력층의 두 단계로 구성
- 다수의 입력을 받아서 하나의 신호를 출력
- 가중치가 곱해진 값들은 모두 더해져 정해진 임계값(threshold)을 넘을 경우에만 다음 노드들이 있는 층(layer)으로 신호가 전해짐
- 입력과 가중치들의 곱을 모두 더한 뒤 활성화 함수를 적용해서 그 값이 0보다 크면 1, 0보다 작으면 -1을 출력하는 선형분류기 구조로 시작
- 단순한 선형 분류기에 불과하여 OR, AND와 같은 분류는 할 수 있으나, XOR 분류를 수행할 수 없다 -> 복잡한 문제를 풀 수 없었다
![](https://i.esdrop.com/d/9760phgt5lnm/zXMjLmkdPo.png)
2. 다층 퍼셉트론(MLP) : 입력층 하나와 은닉층이라 불리는 하나 이상의 TLU층과 마지막 출력층으로 구성된 인공 신경망
- 퍼셉트론의 한계를 극복하기 위해 제시된 방법 -> 퍼셉트론을 여러 층으로 쌓아 XOR 문제를 해결
- 출력층을 제외하고 모든 층은 편향 뉴런을 포함하며 다음 층과 완전히 연결되어 있다
- 퍼셉트론을 여러 층으로 쌓을수록 최적의 가중치를 찾는 것이 어려워지는 문제가 발생 -> 역전파 알고리즘 제안
- 심층 신경망(DNN) : 은닉층을 여러 개 쌓아 올린 인공 신경망
    + 딥러닝은 이 심층 신경망을 연구하는 분야이며 조금 더 일반적으로는 연산이 연속하여 길게 연결된 모델을 연구한다
    + 많은 사람들이 얕더라도 신경망이 사용되면 딥러닝이라고 말함

## 2. 인공 신경망(Artificial neural network)
1. 인공 신경망 : 뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델
- 여러 입력값을 받아서 일정 수준이 넘어서면 활성화되어 출력값을 내보냄
- 딥러닝 분야에서 핵심적
- 다재다능하고 강력하고 확장성이 좋아 복잡한 대규모 머신러닝 문제를 다루는 데 적합
2. 입력층 (Input Layers)
- 입력층은 데이터셋으로부터 입력을 받음
- 입력 변수의 수와 입력 노드의 수는 같다
- 보통 입력층은 어떤 계산도 수행하지 않고 그냥 값들을 전달하기만 함
- 신경망의 층수(깊이, depth)를 셀 때 입력층은 포함하지 않음
3. 은닉층 (Hidden Layers)
- 계산이 없는 입력층과 마지막 출력층 사이에 있는 층
- 은닉층에 있는 계산의 결과를 사용자가 볼 수 없다
- 딥러닝(deep learning)은 사실 두 개 이상의 (이때 부터 깊다(deep)라고 함) 은닉층들을 가진 신경망, 입력층을 제외하고 3개 이상의 Layer를 갖는 신경망을 의미한다
4. 출력층 (Output Layers)
- 신경만 가장 오른쪽 마지막 층
- 출력층에는 대부분 활성함수(activation function)가 존재하는데 활성화함수는 풀고자 하는 문제에 따라 다른 종류를 사용한다
- 회귀 문제에서 예측할 목표 변수가 실수값인 경우 활성화함수가 필요하지 않으며 출력노드의 수는 출력변수의 개수와 같다
- 이진 분류(binary classification) 문제의 경우 시그모이드(sigmoid) 함수를 사용\
    출력을 확률 값으로 변환하여 클래스(Class or label)를 결정함
- 다중클래스(multi-class)를 분류하는 경우 출력층 노드가 부류 수 만큼 존재하며 소프트맥스(softmax) 함수를 사용

## 3. 역전파(Backpropagation)
1. 역전파 훈련 알고리즘 : 효율적인 기법으로 그래디언트를 자동으로 계산하는 경사 하강법
- 네트워크를 두 번 통과하는 것만으로 모든 모델 파라미터에 대한 네트워크 오차의 그래디언트를 계산할 수 있다
- 오차를 감소시키기 위해 각 연결 가중치와 편향값이 어떻게 바뀌어야 할지 알수 있다
- 그래디언트를 구한 후 평범한 경사하강법을 수행한다

- 정방향 계산
> 1. 각 미니배치는 네트워크의 입력층으로 전달되어 첫 번째 은닉층으로 보내진다
> 2. 해당 층에 있는 모든 뉴런의 출력을 계산한다
> 3. 결과를 다음 층으로 전달하고 다시 이 층의 출력을 계산한다
> 4. 마지막 층인 출력층의 출력을 계산할 때까지 반복
> 5. 모든 연결 가중치에 대한 오차 그래디언트를 측정

- 역방향 계산
> 1. 손실 함수를 사용하여 기대하는 출력과 네트워크의 실제 출력을 비교하고 오차 측정 값을 반환
> 2. 각 출력 연결이 이 오차에 기여하는 정도를 계산 (chain rule을 이용하여 빠르고 정확하게 수행)
> 3. 이전 층의 연결 가중치가 이 오차의 기여 정도에 얼마나 기여했는지 측정
> 4. 입력층에 도달할 때까지 반복

- 마지막으로 경사 하강법을 수행하여 계산한 오차 그래디언트를 사용해 네트워크에 있는 모든 연결 가중치를 오차가 감소하도록 수정

2. 활성화 함수 : 복잡한 문제를 풀기 위해서 비선형성을 추가해주는 역할
- 하이퍼볼릭 탄젠트 함수
    + 로지스틱 함수처럼 S자 모양을 가지고 연속적이며 미분 가능하다
    + 출력 범위가 -1에서 1사이 (로지스틱 : 0에서 1사이)
    + 춘련 초기에 각 층의 출력을 원점 근처로 모으는 경향이 있어 빠르게 수렴되도록 도와준다
- ReLU 함수
    + 연속적이지만 z=0에서 미분 불가능 -> 기울기가 갑자기 변하기 때문에 경사하강법이 엉뚱한 곳으로 튈 수 있다
    + z<0일 경우 미분값은 0
    + 잘 작동하고 계산 속도가 빠르다는 장점이 있어 기본 활성화 함수로 사용됨
    + 출력에 최댓값이 없기 때문에 경사 하강법에 있는 일부 문제를 완화해준다
    
    

