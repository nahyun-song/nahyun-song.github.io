---
layout: single
title:  "20211110 CNN"
categories: AI_bootcamp
---

1. CNN 개념
- 사람의 시각 피질 구조, 즉 눈으로 인지된 사진이 뇌의 특정 부분을 자극한다는 이론에서 비롯됨
- 특정 부분의 뇌의 뉴런은 특정 자극을 인지했을 때만 반응함 -> 특정부분 : 국부 수용장(local receptive field)
- 시각 신호가 연속적으로 뉴런들을 지나면서, 더 큰 수용장에 있는 패턴에 반응하는 식으로 인지 -> CNN에 반영
- 장점
    + 이전 이미지 학습 기술에 비해 이미지 전처리 (자르기 / 센터링, 정규화 등)가 상대적으로 거의 필요하지 않다
    + 이와 관련하여 이미지의 모든 종류의 일반적인 문제 (이동, 조명 등)에 대해 견고하다

- Convolutional layer
    - 첫 번째 합성곱 층의 뉴런은 입력 이미지의 모든 픽셀에 연결되는 것이 아니라 합성곱 층 뉴런의 수용장 안에 있는 픽셀에만 연결
    - 두 번째 합성곱 층에 있는 각 뉴런은 첫 번째 층의 작은 사각 영역 안에 위치한 뉴런에 연결\
        이런 구조는 네트워크가 첫 번째 은닉층에서는 작은 저수준 특성에 집중하고, 그다음 은닉층에서는 더 큰 고수준 특성으로 조합해 나가도록 도와줌
    - 다른 다층 신경망과는 달리 각 층이 2D로 표현되므로 뉴런을 그에 상응하는 입력과 연결하기 더 쉽다

    - zero padding : 높이와 너비를 이전 층과 같게 하기 위해 입력의 주위에 0을 추가하는 것
    - stride : 한 수용장과 다음 수용장 사이의 간격
    - filter(convolution kernel) : 가중치 (weights parameters)의 집합으로 이루어져 가장 작은 특징을 잡아내는 창

- Pooling layer - feature map의 차원을 줄임
    - 계산량과 메모리 사용량, 파라미터수를 줄이기 위해 입력 이미지의 subsample을 만드는 것
    - 합성곱 층에서와 마찬가지로 풀링층의 각 뉴런은 이전 층의 작은 사각 영역의 수용장 안에 있는 뉴런의 출력과 연결
    - 풀링 뉴런은 가중치가 없다 -> 최대나 평균 같은 합산 함수를 사용해 입력값을 더함
    - 각 수용장에서 가장 큰 입력값이 다음 층으로 전달되고 다른 값은 버려진다
    - 풀링 층은 보통 모든 입력 채널에 독립적으로 적용되므로 출력의 깊이가 입력의 깊이와 동일함

2. CNN 구조
- 전형적인 CNN 구조는 합성곱 층을 몇 개 쌓고, 그다음에 풀링층을 쌓고, 그다음에 또 합성곱 층을 몇 개 더 쌓고, 그다음에 다시 풀링층을 쌓는 식
- 네트워크를 통과하여 진행할수록 이미지는 점점 작아지지만, 합성곱 층 때문에 일반적으로 점점 더 깊어진다 -> 더 많은 feature map을 갖는다
- 맨 위층에는 몇 개의 완전 연결 층으로 구성된 일반적인 피드포워드 신경망이 추가되고 마지막 층에서 예측을 출력

3. CNN 종류
- AlexNet
    + 합성곱 층 위에 풀링 층을 쌓지 않고 바로 합성곱 층끼리 쌓은 구조
    + 과대적합을 줄이기 위해 Data augmentation 수행
        + Data augmentation : 이미지를 랜덤하게 여러 간격으로 이동하거나 수평으로 뒤집고 조명을 바꾸는 등 샘플을 인공적으로 생성하여 훈련 세트의 크기를 늘리는 것
    + LRN이라 부르는 경쟁적인 정규화 단계 사용 : 가장 강하게 활성화된 뉴런이 다른 특성 맵에 있는 같은 위치의 뉴런을 억제\
        -> 특성 맵을 각기 특별하게 다른 것과 구분되게 하고, 더 넓은 시각에서 특징을 탐색하도록 만들어 일반화 성능을 
        
- GoogLeNet
    + 인셉션 모듈 이라는 서블 네트워크를 가지고 있어 이전의 구조보다 훨씬 효과적으로 파라미터를 사용함\
        AlexNet보다 10배나 적은 파라미터를 가짐
    + 처음에 입력 신호가 복사되어 네 개의 다른 층에 주입됨
    + 모든 합성곱 층은 ReLU 확성화 함수를 사용
    + 두 번째 합성곱 층은 각기 다른 커널 크기를 사용하여 다른 크기의 패턴을 잡음
    + 모든 층은 스트라이드 1과 'same' 패딩을 사용하므로 출력의 높이와 너비가 모두 입력과 같음
    
- VGGNet
    + 매우 단순하고 고전적인 구조
    + 2개 또는 4개의 합성곱 층 뒤에 풀링 층이 나오고 다시 2개 또는 4개의 합성곱 층과 풀링 층이 등장하는 식
    + 종류에 따라 총 16개 또는 19개의 합성곱 층이 있다
    + 마지막 밀집 네트워크는 2개의 은닉층과 출력층으로 이루어짐
    + 많은 필터를 사용하지만 3X3 필터만 사용함

- Batch Normalization
    + 딥러닝에서 레이어가 많아질 때 학습이 어려워지는 이유는 weight의 미세한 변화들이 누적되어 쌓이게 되면, 은닉층이 높아질수록 값의 변화폭이 커짐 
    + 이러한 문제를 해결하기 위해 training 과정 자체를 전체적으로 안정화시킬수 있는 방법
    + weight에 따른 weighted sum의 변화폭이 줄어든다면 학습이 잘 될 것이라는 게 이 방법의 기본 가정
    + 각 층의 입력을 평균 0, 표준편차 1인 분포로 정규화시켜준다
    + 정규화된 값에 scale factor와 shift factor가 추가되고 이 factor는 역전파 알고리즘 과정에서 훈련된다
    + 빠른 학습이 가능하고 가중치의 initialize를 크게 고려하지 않고도 vanishing, exploding gradient 문제를 막을 수 있음
    + hidden layer에 들어가기 전에 batch normalization layer를 추가하여 입력의 분포를 바꾼 뒤 활성화함수를 넣어주는 방식으로 이용
- Dropout
    + 과대적합을 막기 위해 네트워크의 일부를 생락하는 것
    + 생략된 일부 네트워크는 학습에 영향을 끼치지 않음
    + 여러 개의 모델을 각각 훈련하는 과정 없이, 일부 네트워크를 생략하여 다른 모델을 학습한 것과 같은 효과를 얻을 수 있음
    + hidden unit의 활성도가 조절되어 서로 간의 correlation이 낮은 feature를 얻을 수 있다
