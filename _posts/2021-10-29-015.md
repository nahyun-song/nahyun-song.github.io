---
layout: single
title:  "20211029 트리/랜덤포레스트/앙상블"
categories: AI_bootcamp
---

# 1. 결정트리 (Decision Trees)

- SVM처럼 분류와 회귀 작업 그리고 다중출력 작업도 가능한 다재다능한 머신러닝 알고리즘
- 매우 복잡한 데이터셋도 학습할 수 있는 강력한 알고리즘임
- 새로운 데이터가 특정 말단 노드에 속한다는 정보를 확인한 뒤 말단 노드의 빈도가 가장 높은 범주로 데이터를 분류함
- 비선형, 비단조, 특성상호작용 특징을 가진 데이터 분석에 용이함
    + 특성상호작용 : 회귀분석에서는 서로 상호작용이 높은 특성들이 있으면 개별 계수를 해석하는데 어려움이 있고, 학습이 올바르게 되지 않을 수 있지만 트리모델은 이런 상호작용을 자동으로 걸러냄
- 데이터 전처리가 거의 필요 없음 (스케일링 등)

![](https://i.esdrop.com/d/9760phgt5lnm/kTh2cJNiOd.png)

1. `criterion` 매개변수
- 엔트로피 : 분자의 무질서함을 측정하는 것. 머신러닝에서는 불순도의 측정 방법으로 자주 사용된다\
   ex)어떤 세트가 한 클래스의 샘플만 담고 있다면 엔트로피=0\
   ![](https://i.esdrop.com/d/9760phgt5lnm/ErpKmY2Du0.png)
- 지니불순도 : 노드의 불순도를 나타내는 값\
   ex)한 노드의 모든 샘플이 같은 클래스에 속해 있다면 gini=0\
   ![](https://i.esdrop.com/d/9760phgt5lnm/yJL7Kvy5xt.png)
- 만들어지는 트리는 거의 비슷하지만 지니 불순도가 조금 더 계산이 빠르기 때문에 기본값으로 좋다
- 서로 다른 트리가 만들어지는 경우 지니 불순도가 가장 빈도 높은 클래스를 한쪽 가지로 고립시키는 경향이 있는 반면 엔트로피는 조금 더 균형 잡힌 트리를 만든다.

2. 규제 매개변수
- 결정 트리는 훈련 데이터에 대한 제약 사항이 거의 없다 -> 과대적합되기 쉽다
- 훈련되기 전에 파라미터 수가 결정되지 않는 비파라미터 모델이다
- `max_depth` : 트리의 최대 깊이 제어. 줄이면 모델을 규제하게 되고 과대적합의 위험이 감소한다
- `min_samples_leaf` : 말단 노드에 최소한 존재해야하는 샘플들의 수를 제어
- `min_samples_split` : 노드를 나누기 위한 최소 샘플 개수

3. 특성중요도
- 선형모델에서는 특성과 타겟의 관계를 확인하기 위해 회귀 계수를 구함
- 결정트리에서는 특성중요도를 확인할 수 있는데 이를 통해 특성이 얼마나 일찍 그리고 자주 분류에 사용되는지 결정된다
    + 중요한 몇 개의 특성이 명확한 트리를 만드는 데 기여한다
    + 특성중요도는 `DecisionTreeClassifier` 객체의 `feature_importances_` 속성을 통해 확인할 수 있음
    + 회귀 계수와는 달리 항상 양수값을 가짐

4. 회귀
- 분류 트리와 차이점
    + 각 노드에서 클래스를 예측하는 대신 어떤 값을 예측한다
    + 불순도를 최소화하는 방향으로 분할하는 대신 평균제곱오차를 최소화하도록 분할한다
- 선형 회귀와 차이점
    + 의사결정 트리의 장점과 수치 데이터를 모델링하는 능력을 결합
    + 일부 데이터 타입에 대해 선형회귀보다 아주 잘 맞고 모델 해석에 통계지식이 필요하지 않다
    + 많은 양의 훈련 데이터가 필요함
    + 결과에 대한 개별 특징의 전체적인 순영향을 알기 어렵다
    + 큰 트리는 회귀 모델보다 해석하기 어려울 수 있다
    + 변수가 많을 경우 시각화하기 어렵다
    + 모델이 가진 데이터 범위 밖으로 나가면 단순히 마지막 포인트를 이용해 예측 -> 트리 모델은 훈련 데이터 밖의 새로운 데이터를 예측할 능력이 없다
    
5. 단점
- 결정트리는 한 개의 트리만을 사용하여 한 노드에서 생김 에러가 하부 노드에서도 계속 영향을 주고, 트리의 깊이에 따라 과적합 되기 쉽다 -> 랜덤포레스트 모델 사용
- 결정 트리는 계단 모양의 결정 경계를 만들기 때문에 훈련 세트의 회전에 민감하다\
   -> PCA 기법으로 문제 해결


# 2. 앙상블 (Ensemble)

- 앙상블 : 여러 머신러닝 모델을 연결하여 더 강력한 모델을 만드는 기법
    1. Bagging (Bootstrap aggregating) : 훈련 세트에서 중복을 허용하여 샘플링하는 방식 (복원추출)
    - 데이터가 충분히 크다고 가정했을 때 한 부트스트랩 세트는 표본의 63.2%에 해당하는 샘플을 가진다
    - 여기서 추출되지 않는 나머지 샘플이 Out-Of-Bag 샘플이며 이 샘플을 사용해 모델을 검증할 수 있다
    - 회귀문제일 경우 기본모델과 결과들의 평균을 내어 기본모델들을 합침
    - 분류문제일 경우 다수결로 가장 많은 모델들이 선택한 범주로 예측
    2. Pasting : 훈련 세트에서 중복을 허용하지 않고 샘플링 하는 방식
    3. Boosting : 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법
    - 에이다부스트 : 이전 모델이 과소적합했던 훈련 샘플의 **가중치**를 더 높여 학습하기 어려운 샘플에 맞춰가는 방식
        > 1. 알고리즘이 기반이 되는 첫 번째 분류기를 훈련 세트에서 훈련시키고 예측을 만든다
        > 2. 알고리즘이 잘못 분류된 훈련 샘플의 가중치를 상대적으로 높인다
        > 3. 다음 분류기는 업데이트된 가중치를 사용해 훈련세트에서 훈련하고 다시 예측을 만든다
        > 4. 다시 가중치를 업데이트하여 위 과정 반복
        - 연속된 학습 기법은 경사하강법과 비슷한 면이 있지만 경사 하강법은 비용함수를 최소화하기 위해 한 예측기의 모델 파라미터를 조정해가는 반면 에이다부스트는 점차 더 좋아지도록 앙상블에 예측기를 추가한다
        - 연속된 학습 기법의 단점 : 각 예측기는 이전 예측기가 훈련되고 평가된 후에 학습될 수 있기 때문에 병렬화(또는 분할)를 할 수 없다
    - 그래디언트 부스팅 : 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가한다. 이전 예측기가 만든 **잔여 오차**에 새로운 예측기를 학습시킴
        + XGBoost(extreme gradient boosting) : 빠른 속도, 확장성, 이식성을 가진 패키지. 자동 조기 종료와 같은 좋은 기능들을 제공함

# 3. 랜덤포레스트 (Random Forest)

- 일반적으로 배깅 방법(또는 페이스팅)을 적용한 결정 트리의 앙상블
- 결정트리는 한 개의 트리만을 사용하여 한 노드에서 생김 에러가 하부 노드에서도 계속 영향을 주고, 트리의 깊이에 따라 과적합 되는 경향이 있다는 단점을 가지고 있는데 랜덤포레스트 모델로 이러한 문제를 해결할 수 있다
- 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식으로 무작위성을 더 주입한다
    + 랜덤포레스트에서 학습되는 트리들은 배깅을 통해 만들어진다. 이때 각 기본트리에 사용되는 데이터가 랜덤으로 선택됨
    + 각각 트리는 무작위로 선택된 특성들을 가지고 분류를 수행함
    + 결국 트리를 더욱 다양하게 만들고 편향을 손해보는 대신 분산을 낮추어 전체적으로 더 훌륭한 모델을 만들어 냄
- 가장 강력한 머신러닝 알고리즘 중 하나
- 전형적으로 `max_samples`를 훈련 세트의 크기로 지정
- 특성의 상대적 중요도를 측정하기 쉽다
    + 중요도는 노드들의 지니불순도를 가지고 계산 - 노드가 중요할수록 불순도가 크게 감소한다는 사실을 이용



















