---
layout: single
title:  "20211022 Clustering"
categories: AI_bootcamp
---


# 1. 머신러닝의 분류 및 종류

1. 강화 학습 : 머신러닝의 한 형태로, 기계가 좋은 행동에 대해서는 보상, 그렇지 않은 행동에는 처벌이라는 피드백을 통해 행동에 대해 학습해 나가는 형태
2. 지도 학습 : 트레이닝 데이터에 레이블이 있을 때 사용
   - 분류 : 주어진 데이터의 카테고리 혹은 클래스 예측
   - 회귀 : 연속인 데이터를 바탕으로 결과를 예측
3. 비지도 학습 : 레이블이 없는 데이터를 알고리즘에 제공하고 구조를 찾는 것
   - 차원 축소 : 높은 차원을 갖는 데이터셋을 사용하여 차원을 줄이는 방법
   - 연관 규칙 학습 : 데이터셋의 feature들의 관계를 발견하는 방법

# 2. 클러스터링 (Clustering)

1. 클러스터링의 목적 : 주어진 데이터들이 얼마나, 어떻게 유사한지 파악하는 것 -> 데이터를 요약/정리하는 데 있어 매우 효율적인 방법 중 하나
2. 클러스터링의 단점 
  - 몇 개의 클러스터가 정확한지 알 수 없다. 정답을 보장할 수 없다. -> 모델링에 쓰기이 보다는 EDA를 위한 방법으로 쓰임
  - 섞여있는 데이터르 분류만 해주고 해석까지 하지는 않는다.
  - 클러스터링의로 분류하는 것은 지도학습의 분류모델만큼의 성능을 발휘하지 못한다.
3. 클러스터링의 종류
 - Hierarchical
    + Agglomerative: 개별 포인트에서 시작후 점점 크게 합쳐감 / 각각의 데이터 포인트를 하나의 클러스터로 지정하고, 지정된 개수의 클러스터가 남을 때까지 가장 비슷한 두 클러스터를 합쳐 나가는 알고리즘 -> 토너먼트와 같은 모양으로 그래프가 나타남
    + Divisive: 한개의 큰 cluster에서 시작후 점점 작은 cluster로 나눠감
 - Point Assignment
    + 시작시에 cluster의 수를 정한 다음, 데이터들을 하나씩 cluster에 배정시킴         
 - Hard vs Soft Clustering
    + Hard Clustering : 데이터는 하나의 cluster에만 할당
    + Soft Clustering : 데이터는 여러 cluster에 확률을 가지고 할당 ex) Fuzzy C-means (softmax 분류와 유사한 아이디어)\
      -> 일반적으로 Hard Clustering을 Clustering이라 칭함        
4. 클러스터링 예시 : 고객 데이터베이스에서 고객을 시장별로 그룹화하는 마켓 세그멘테이션, 소셜 네트워크에서 일관성을 찾는 것 등
5. 클러스터링의 성능을 최적화하는 방법
   - 엘보우 메서드(Elbow Method)
     + K-means를 통해 구한 군집 내에서, k의 변화에 따른 SSE를 관찰한다.
     + 왜곡이 가장 빨리 증가하는 시점의 k를 식별함으로써 최적의 군집 개수를 검출한다.
   - 실루엣 분석(Silhouette analysis)
     + 클러스터링의 성능 자체를 수량화하는 것
     + 군집 내의 샘플이 얼마나 결합력 있게 그룹핑 되었는지를 수량화하여 평가

# 3. K-means clustering

1. K-means clustering : 사용자가 군집의 개수(k개)를 미리 지정하는 모델
 - 모집단, 범주에 대한 정보가 없을 때 사용
 - 새로운 데이터와 기존 데이터 간의 유클리디안 거리가 최소가 되도록 클러스터링
 - 데이터들을 k개의 클러스터로 군집화
 - 중심값에서 중심과의 거리를 비교 (거리 차이의 분산을 최소화)
     + 맨하탄 거리 : 각 축에 대해 수직을 이동하면서 계산하는 거리측정 방식
     + 유클리디안 거리 : 가장 짧은 거리를 계산하는 거리측정 방식

    >n-차원의 데이터에 대해서 :

    >1. k 개의 랜덤한 데이터를 cluster의 중심점으로 설정
    >2. 해당 cluster에 근접해 있는 데이터를 cluster로 할당
    >3. 변경된 cluster에 대해서 중심점을 새로 계산

    >cluster에 유의미한 변화가 없을 때 까지 2-3을 반복
    
<img src='https://url.kr/a379us'>

  - K-means에서 K를 결정하는 방법
     + The Eyeball Method :사람의 주관적인 판단을 통해서 임의로 지정하는 방법
     + Metrics : 객관적인 지표를 설정하여, 최적화된 k를 선택하는 방법

  - 장점
     + 구현이 쉽고 계산이 효율적임
     + 새로운 자료 없이 스스로 학습 가능
  - 단점
     + k를 직접 정해야 함 -> 최적의 k를 찾는다고 해도 정확한 분류가 어려울 수 있음
     + 거리를 기준으로 분류를 하기 때문에 거리의 개념이 없어지는 고차원의 경우에는 제대로된 성능이 나오지 않음
     + 랜덤으로 잡힌 초기 중간값에 민감
     + 거리의 평균값을 다루기 때문에 이상치 데이터에 민감
     
  - 적용분야
     + 성향이 불분명한 시장 분석
     + 트렌드와 같이 명확하지 못한 기준 분석
     + 패턴인식, 음성인식 기본 기술
     + 관련성을 알 수 없는 데이터 초기 분류\
       ->네트워크 유해 트래픽 탐지, 영화나 TV 장면의 분류, SNS 유사 사용자 군집화, 공시지가 유사가격 권역 설정 등
  
2. python 실습


```python
import pandas as pd
from sklearn.cluster import KMeans 
from sklearn.preprocessing import StandardScaler 
from sklearn.datasets import make_blobs

x, y = make_blobs(n_samples = 100, centers = 3, n_features = 2)
df = pd.DataFrame(dict(x = x[:, 0], y = x[:, 1], label = y))
df
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-7.840686</td>
      <td>-3.222411</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-4.895048</td>
      <td>3.727969</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-6.104230</td>
      <td>2.082450</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-7.577840</td>
      <td>-3.677294</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-7.256211</td>
      <td>-3.023476</td>
      <td>2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>95</th>
      <td>10.095811</td>
      <td>6.607261</td>
      <td>0</td>
    </tr>
    <tr>
      <th>96</th>
      <td>-4.571323</td>
      <td>3.262280</td>
      <td>1</td>
    </tr>
    <tr>
      <th>97</th>
      <td>-5.480666</td>
      <td>4.288329</td>
      <td>1</td>
    </tr>
    <tr>
      <th>98</th>
      <td>-6.437721</td>
      <td>2.919255</td>
      <td>1</td>
    </tr>
    <tr>
      <th>99</th>
      <td>-4.545966</td>
      <td>3.240209</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>100 rows × 3 columns</p>
</div>




```python
#scatter plot으로 데이터 살펴보기
import matplotlib.pyplot as plt

points = df.drop('label', axis = 1) # label 삭제 
plt.scatter(points.x, points.y)
plt.show()
```


    
![png](output_7_0.png)
    



```python
#k를 3으로 설정할 수 있을 것 같지만 한번 더 확인
```


```python
#elbow method로 최적의 k찾기
sum_of_squared_distances = []
K = range(1, 15)
for k in K:
    km = KMeans(n_clusters = k)
    km = km.fit(points)
    sum_of_squared_distances.append(km.inertia_)
```


```python
plt.plot(K, sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k')
plt.show()
```


    
![png](output_10_0.png)
    



```python
#급격히 꺾이는 부분을 확인해보면 k=3이 적절
```


```python
# k-means clustering / n_clusters = 3, random_state = 42
kmeans = KMeans(n_clusters = 3, random_state = 42)
kmeans.fit_transform(points)
labels = kmeans.labels_
labels
```




    array([1, 2, 2, 1, 1, 2, 0, 2, 2, 2, 1, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1,
           1, 1, 0, 2, 0, 2, 0, 0, 0, 2, 1, 1, 0, 2, 0, 2, 1, 1, 1, 2, 0, 2,
           1, 1, 2, 0, 1, 2, 0, 1, 1, 2, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 2,
           0, 0, 1, 1, 1, 2, 0, 1, 2, 0, 1, 2, 1, 0, 0, 2, 0, 0, 1, 1, 1, 2,
           2, 0, 2, 0, 1, 2, 2, 0, 2, 2, 2, 2], dtype=int32)




```python
points['clusters'] = labels
points.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
      <th>clusters</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-7.840686</td>
      <td>-3.222411</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-4.895048</td>
      <td>3.727969</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-6.104230</td>
      <td>2.082450</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-7.577840</td>
      <td>-3.677294</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-7.256211</td>
      <td>-3.023476</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
#중심점 찾기
centroids = points.groupby('clusters').mean()
centroids
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
    </tr>
    <tr>
      <th>clusters</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9.628881</td>
      <td>7.465280</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-8.569678</td>
      <td>-3.231758</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-5.325064</td>
      <td>3.113077</td>
    </tr>
  </tbody>
</table>
</div>




```python
#그래프로 확인
colors = {0 : '#E9687E', 1 : '#FDC2B1', 2 : '#F7E298'}
fig, ax = plt.subplots()
ax.plot(centroids.iloc[0].x, centroids.iloc[0].y, "ok") # 기존 중심점
ax.plot(centroids.iloc[1].x, centroids.iloc[1].y, "ok")
ax.plot(centroids.iloc[2].x, centroids.iloc[2].y, "ok")
grouped = points.groupby('clusters')
for key, group in grouped:
  group.plot(ax = ax, kind = 'scatter', x = 'x', y = 'y', label = key, color = colors[key])
plt.show()
```


    
![png](output_15_0.png)
    


# 4. Hierarchical clustering

1. Hierarchical clustering
   - 거리 또는 유사도를 기반으로 개별 개체들을 순차적으로, 그리고 계층적으로 유사한 그룹을 통합하는 알고리즘
   - K-means와는 다르게 군집 수를 사전에 정해주지 않아도 됨
   - 군집 간의 연결법에 따라 군집의 결과가 달라질 수 있음
   - 덴드로그램의 형태로 결과가 주어지며 각 개체는 하나의 군집에만 속하게 됨
      + 덴드로그램 : 개체 간의 계층적 관계를 보여주는 다이어그램. 해석의 핵심은 두 물체가 결합된 높이에 초점을 맞추는 것
   - 계산복잡도는 O(n^3)으로 K-means 보다는 무거운 편 (K-means의 계산복잡도 = O(n))
   
   > 1. 각 데이터들을 Cluster라고 가정
   > 2. 각 데이터 pair 사이의 distance를 계산하여 가장 작은 pair를 하나의 cluster로 묶음
   > 3. 모든 데이터가 하나의 cluster로 묶일 때까지 반복

   - 장점
      + 시각적으로 덴드로그램을 제공하여 보기 편함
      + 넓게 사용되고 쉽다
      
   - 단점
      + 데이터가 많으면 계산의 제한이 있다
      + 데이터 전처리에서 잘못된 것을 고칠 수 없다
   

2. python 실습


```python
from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt

# Calculate the linkage: mergings
mergings = linkage(points, method='complete')

# Plot the dendrogram, using varieties as labels
plt.figure(figsize=(40,20))
dendrogram(mergings,
           leaf_rotation=90,
           leaf_font_size=10,)
plt.show()
```


    
![png](output_19_0.png)
    

