---
layout: single
title:  "20220222 week1-2"
categories: preonboarding
---

# Paraphrase Identification

- 문제 정의
    - 문장 쌍이 동일한 의미를 가지는지 여부를 결정 (binary classification)
  
- 데이터셋 소개 : QQP(Quora Question Pairs)
    - 400,000개 이상의 질문 쌍으로 구성
    - 각 질문 쌍에는 두 질문이 서로 패러프레이즈인지 여부를 나타내는 이진 값으로 라벨링 되어 있다
    - 아래 예시에서 딥러닝 모델은 문장 1과 문장 2가 같은 의미의 문장인지 아닌지에 대한 class label을 기반으로 weight propagation을 수행하여 학습하게 된다

![](https://i.esdrop.com/d/f/Wt5zrblYBe/W1aPfjZ3Oy.png)
    
- SOTA 모델 소개 (가장 performance가 좋은 모델은 어제 소개했던 XLNet이라 2위와 3위 모델을 소개함)
    1. DeBERTa
        - 구글의 버트(BERT)와 같은 사전 훈련 언어 모델(PLM)
        - BERT 및 RoBERTa 모델을 개선한 모델 (디코딩을 강화한 BERT)
            1. 분리 주의 매커니즘 : 각 단어는 각각 내용과 위치를 인코딩하는 두 벡터를 사용하여 표현되며 단어 간의 주의 가중치는 각각 내용 및 상대 위치에 대한 분리 행렬을 사용하여 계산된다
            2. 향상된 마스크 언어모델 : 모델 사전 교육에서 마스크된 토큰을 예측하기 위해 디코딩 계층에 absolute position을 통합하는 데 사용된다
            3. 스케일 인베리언트 파인 튜닝 : 모델의 일반화를 개선하기 위해 미세 조정에 새로운 가상 적대적 훈련 방법이 사용된다
        - AI가 인간의 언어를 얼마나 잘 이해하고 있는지를 평가하는 척도인 슈퍼글루(SuperGLUE) 점수가 2021년 초 90.3을 기록하며 인간의 수준(89.8)을 넘어섰다

    2. ALBERT
        - BERT는 Per-trained 모델이기 때문에 크기가 커질수록 성능이 향상된다. 하지만, 모델이 커지면 아래의 문제가 발생한다.
            * Memory Limitation : 메모리 양이 모델의 크기에 비해 작으면, out of memory 발생
            * Training Time : 학습에 필요한 시간이 너무 오래 걸림
            * Memory Degradation : Layer 혹은 Hidden Size가 커지면 모델 성능 오히려 감소
        - 메모리 소비를 줄이고 BERT의 훈련 속도를 높이기 위한 두 가지 매개 변수 감소 기술을 제시
            1. Factorized Embedding Parameterization :  Input Layer의 파라미터 수를 줄여서 모델 크기를 줄인다
            2. Cross-layer Parameter Sharing : Transformer의 각 Layer 간 같은 파라미터를 공유하면서 모델 크기를 줄인다
            3. Sentence Order Prediction : BERT의 NSP(Next Sentence Prediction) 대신 SOP(Sentence Order Prediction)으로 학습
